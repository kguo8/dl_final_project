{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels=[3, 40, 60, 120, 160, 240], kernel=5, padding=2, pool_kernel=2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels[0], channels[1], kernel, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(channels[1], channels[2], kernel, padding=padding)\n",
    "        self.conv3 = nn.Conv2d(channels[2], channels[3], kernel, padding=padding)\n",
    "        self.conv4 = nn.Conv2d(channels[3], channels[4], kernel, padding=padding)\n",
    "        self.conv5 = nn.Conv2d(channels[4], channels[5], kernel, padding=padding)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        # setting stride to equal kernel\n",
    "        self.pool = nn.MaxPool2d(pool_kernel, stride=pool_kernel)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv2(self.relu(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.conv4(self.relu(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        return self.relu(self.conv5(x))\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels=[240, 120, 60, 2, 1], kernel=5, padding=2, mid_kernel=2):\n",
    "        super().__init__()\n",
    "        self.deconv1 = nn.ConvTranspose2d(channels[0], channels[1], kernel, padding=padding)\n",
    "        # kernel and stride to match the pool layer in the encoder\n",
    "        self.deconv2 = nn.ConvTranspose2d(channels[1], channels[2], mid_kernel, stride=mid_kernel)\n",
    "        self.deconv3 = nn.ConvTranspose2d(channels[2], channels[3], mid_kernel, stride=mid_kernel)\n",
    "        # for generating output (out channel is 1 mask is one layer)\n",
    "        self.deconv4 = nn.ConvTranspose2d(channels[3], channels[4], kernel, padding=padding)\n",
    "    \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.deconv1(x))\n",
    "        x = self.relu(self.deconv2(x))\n",
    "        return self.deconv4(self.deconv3(x)).squeeze()\n",
    "\n",
    "class WickUnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import albumentations.augmentations.functional as F\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_names, transform, test=False):\n",
    "        self.image_names = image_names\n",
    "        self.transform = transform\n",
    "        self.test = test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(f'prima/{self.image_names[idx]}.tif')\n",
    "        mask = scipy.sparse.load_npz(f'prima/mask_{self.image_names[idx]}.npz').todense()\n",
    "        \n",
    "        transformed = self.transform(image=img, mask=mask)\n",
    "        \n",
    "        if self.test:\n",
    "            img_size = img.shape[:2]\n",
    "            return transformed[\"image\"], transformed[\"mask\"], img_size\n",
    "        return transformed[\"image\"], transformed[\"mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(392, 260),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transform = A.Compose(\n",
    "    [\n",
    "         A.Resize(392, 260),\n",
    "         A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "         ToTensorV2()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files = ['00008152', '00008153', '00008150', '00008148', '00325453',\n",
    "'00008063', '00008230', '00322470', '00325454', '00008332',\n",
    "'00008062', '00008143', '00008344', '00008141', '00008146',\n",
    "'00008088', '00008346', '00325450', '00008149', '00008147',\n",
    "'00325448', '00008227', '00008151', '00322599', '00322596',\n",
    "'00008334', '00008336', '00008089', '00008229', '00322468',\n",
    "'00008144', '00008340', '00008061', '00008084', '00325449',\n",
    "'00322471', '00008140', '00322469', '00008086', '00008145']\n",
    "\n",
    "test_files = ['00008228',\n",
    " '00322597',\n",
    " '00008338',\n",
    " '00008064',\n",
    " '00322598',\n",
    " '00325451',\n",
    " '00008142',\n",
    " '00325452',\n",
    " '00008154',\n",
    " '00008342']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = np.random.choice(training_files, 32, replace=False)\n",
    "val_files = list(set(training_files) - set(train_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ImageDataset(train_files, train_transform)\n",
    "val_ds = ImageDataset(val_files, val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_dl = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, loss_fun, optimizer):\n",
    "    total_loss = 0\n",
    "    total_row = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for img, mask in dataloader:\n",
    "        pred = model(img)\n",
    "        loss = loss_fun(pred, mask.float())\n",
    "\n",
    "        total_loss += loss.item() * img.shape[0]\n",
    "        total_row += img.shape[0]\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / total_row\n",
    "\n",
    "def validate(model, dataloader, loss_fun):\n",
    "    total_loss = 0\n",
    "    total_row = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, mask in dataloader:\n",
    "            pred = model(img)\n",
    "            loss = loss_fun(pred, mask.float())\n",
    "\n",
    "            total_loss += loss.item() * img.shape[0]\n",
    "            total_row += img.shape[0]\n",
    "\n",
    "    return total_loss / total_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WickUnet()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "loss_fun = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = train(model, train_dl, loss_fun, optimizer)\n",
    "vl = validate(model, val_dl, loss_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7293292880058289, 0.6918942332267761)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl, vl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('prima/00008062.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_img = val_transform(image=img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(transformed_img['image'].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.3) /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/pip-req-build-gt63l4kp/opencv/modules/imgproc/src/thresh.cpp:1557: error: (-2:Unspecified error) in function 'double cv::threshold(cv::InputArray, cv::OutputArray, double, double, int)'\n> THRESH_OTSU mode:\n>     'src_type == CV_8UC1 || src_type == CV_16UC1'\n> where\n>     'src_type' is 5 (CV_32FC1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-245-323662f51b1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTHRESH_BINARY\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTHRESH_OTSU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.3) /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/pip-req-build-gt63l4kp/opencv/modules/imgproc/src/thresh.cpp:1557: error: (-2:Unspecified error) in function 'double cv::threshold(cv::InputArray, cv::OutputArray, double, double, int)'\n> THRESH_OTSU mode:\n>     'src_type == CV_8UC1 || src_type == CV_16UC1'\n> where\n>     'src_type' is 5 (CV_32FC1)\n"
     ]
    }
   ],
   "source": [
    "T, thresh = cv2.threshold(output_np, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
