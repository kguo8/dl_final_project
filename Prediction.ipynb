{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
   "id": "english-insured",
=======
   "execution_count": 3,
>>>>>>> e27d602f257c2b6af901081164d3f4e5c6c74fcc
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import albumentations.augmentations.functional as F\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
   "id": "united-technician",
=======
   "execution_count": 4,
>>>>>>> e27d602f257c2b6af901081164d3f4e5c6c74fcc
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_names, transform, img_size=False):\n",
    "        self.image_names = image_names\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(f'prima/{self.image_names[idx]}.tif')\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
<<<<<<< HEAD
    "        mask = scipy.sparse.load_npz(f'mask/mask_{self.image_names[idx]}.npz').todense()\n",
=======
    "        mask = scipy.sparse.load_npz(f'prima/mask_{self.image_names[idx]}.npz').todense()\n",
>>>>>>> e27d602f257c2b6af901081164d3f4e5c6c74fcc
    "        \n",
    "        transformed = self.transform(image=img, mask=mask)\n",
    "        \n",
    "        if self.img_size:\n",
<<<<<<< HEAD
    "            h, w = img.shape[:2]\n",
    "            return transformed[\"image\"], h, w\n",
=======
    "            return transformed[\"image\"], transformed[\"mask\"], img.shape[:2]\n",
>>>>>>> e27d602f257c2b6af901081164d3f4e5c6c74fcc
    "        return transformed[\"image\"], transformed[\"mask\"]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "id": "catholic-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = ['00008228',\n",
=======
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = {'00008228',\n",
>>>>>>> e27d602f257c2b6af901081164d3f4e5c6c74fcc
    " '00322597',\n",
    " '00008338',\n",
    " '00008064',\n",
    " '00322598',\n",
    " '00325451',\n",
    " '00008142',\n",
    " '00325452',\n",
    " '00008154',\n",
<<<<<<< HEAD
    " '00008342']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intense-luxembourg",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels=[3, 40, 60, 120, 160, 240], kernel=5, padding=2, pool_kernel=2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels[0], channels[1], kernel, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(channels[1], channels[2], kernel, padding=padding)\n",
    "        self.conv3 = nn.Conv2d(channels[2], channels[3], kernel, padding=padding)\n",
    "        self.conv4 = nn.Conv2d(channels[3], channels[4], kernel, padding=padding)\n",
    "        self.conv5 = nn.Conv2d(channels[4], channels[5], kernel, padding=padding)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        # setting stride to equal kernel\n",
    "        self.pool = nn.MaxPool2d(pool_kernel, stride=pool_kernel)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv2(self.relu(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.conv4(self.relu(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        return self.relu(self.conv5(x))\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels=[240, 120, 60, 2, 1], kernel=5, padding=2, mid_kernel=2):\n",
    "        super().__init__()\n",
    "        self.deconv1 = nn.ConvTranspose2d(channels[0], channels[1], kernel, padding=padding)\n",
    "        # kernel and stride to match the pool layer in the encoder\n",
    "        self.deconv2 = nn.ConvTranspose2d(channels[1], channels[2], mid_kernel, stride=mid_kernel)\n",
    "        self.deconv3 = nn.ConvTranspose2d(channels[2], channels[3], mid_kernel, stride=mid_kernel)\n",
    "        # for generating output (out channel is 1 mask is one layer)\n",
    "        self.deconv4 = nn.ConvTranspose2d(channels[3], channels[4], kernel, padding=padding)\n",
    "    \n",
    "        self.relu = nn.ReLU()\n",
    "      \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.deconv1(x))\n",
    "        x = self.relu(self.deconv2(x))\n",
    "        return self.deconv4(self.deconv3(x)).squeeze()\n",
    "\n",
    "class WickUnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))"
=======
    " '00008342'}"
>>>>>>> e27d602f257c2b6af901081164d3f4e5c6c74fcc
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
   "id": "retained-miller",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('rotate_noise_epoch_90.pt', map_location=torch.device('cpu'))"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('')"
>>>>>>> e27d602f257c2b6af901081164d3f4e5c6c74fcc
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
   "id": "overhead-copying",
=======
   "execution_count": 16,
>>>>>>> e27d602f257c2b6af901081164d3f4e5c6c74fcc
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = A.Compose(\n",
    "    [\n",
    "         A.PadIfNeeded(min_height=10296, min_width=7020),\n",
    "         A.Resize(396, 264),\n",
    "         A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "         ToTensorV2()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
   "id": "seven-money",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ImageDataset(list(test_files), val_transform, img_size=True)\n",
    "val_dl = DataLoader(ds, batch_size = 1, shuffle=False)"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ImageDataset(list(test_files), val_transform, img_size=True)"
>>>>>>> e27d602f257c2b6af901081164d3f4e5c6c74fcc
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 31,
   "id": "honest-uncertainty",
=======
   "execution_count": null,
>>>>>>> e27d602f257c2b6af901081164d3f4e5c6c74fcc
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "    \n",
    "with torch.no_grad():\n",
<<<<<<< HEAD
    "    for i,(img, h,w) in enumerate(val_dl):\n",
    "        pred = model(img)\n",
    "        \n",
    "        probabilities = torch.sigmoid(pred)\n",
=======
    "    for i in range(len(ds)):\n",
    "        img, mask, (h, w) = ds[i]\n",
    "#        img = img.cuda()\n",
    "#        mask = mask.cuda()\n",
    "\n",
    "        pred = model(img)\n",
    "        \n",
    "        probabilities = torch.sigmoid(pred.squeeze(1))\n",
>>>>>>> e27d602f257c2b6af901081164d3f4e5c6c74fcc
    "        predicted_masks = (probabilities >= 0.5).float().detach().numpy()\n",
    "\n",
    "        aug = A.Compose(\n",
    "            [\n",
    "                A.Resize(10296, 7020),\n",
    "                A.CenterCrop(h, w)\n",
    "            ]\n",
    "        )\n",
<<<<<<< HEAD
    "        del probabilities\n",
    "        transformed = aug(image=predicted_masks)\n",
    "        \n",
    "        sparse_mask = scipy.sparse.csr_matrix(transformed['image'])\n",
    "        scipy.sparse.save_npz(f'mask/pred_mask_{test_files[i]}.npz', sparse_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "defined-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "organizational-medication",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31726370940080156"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = 0\n",
    "for file in test_files:\n",
    "    y = scipy.sparse.load_npz(f'mask/mask_{file}.npz').todense()\n",
    "    pred = scipy.sparse.load_npz(f'mask/pred_mask_{file}.npz').todense()\n",
    "    f1 += f1_score(np.array(y).flatten(),np.array(pred).flatten().astype(int))\n",
    "f1 = f1/len(test_files)    \n",
    "f1    "
=======
    "        \n",
    "        original = aug(image=img, mask=pred)\n",
    "        \n",
    "        sparse_mask = sparse.csr_matrix(original['mask'])\n",
    "        scipy.sparse.save_npz(f'prima/pred_mask_{file_name}.npz', sparse_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
>>>>>>> e27d602f257c2b6af901081164d3f4e5c6c74fcc
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
=======
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
>>>>>>> e27d602f257c2b6af901081164d3f4e5c6c74fcc
}
