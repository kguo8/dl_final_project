{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels=[3, 40, 60, 120, 160, 240], kernel=5, padding=2, pool_kernel=2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels[0], channels[1], kernel, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(channels[1], channels[2], kernel, padding=padding)\n",
    "        self.conv3 = nn.Conv2d(channels[2], channels[3], kernel, padding=padding)\n",
    "        self.conv4 = nn.Conv2d(channels[3], channels[4], kernel, padding=padding)\n",
    "        self.conv5 = nn.Conv2d(channels[4], channels[5], kernel, padding=padding)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        # setting stride to equal kernel\n",
    "        self.pool = nn.MaxPool2d(pool_kernel, stride=pool_kernel)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv2(self.relu(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.conv4(self.relu(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        return self.relu(self.conv5(x))\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels=[240, 120, 60, 2, 1], kernel=5, padding=2, mid_kernel=2):\n",
    "        super().__init__()\n",
    "        self.deconv1 = nn.ConvTranspose2d(channels[0], channels[1], kernel, padding=padding)\n",
    "        # kernel and stride to match the pool layer in the encoder\n",
    "        self.deconv2 = nn.ConvTranspose2d(channels[1], channels[2], mid_kernel, stride=mid_kernel)\n",
    "        self.deconv3 = nn.ConvTranspose2d(channels[2], channels[3], mid_kernel, stride=mid_kernel)\n",
    "        # for generating output (out channel is 1 mask is one layer)\n",
    "        self.deconv4 = nn.ConvTranspose2d(channels[3], channels[4], kernel, padding=padding)\n",
    "    \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.deconv1(x))\n",
    "        x = self.relu(self.deconv2(x))\n",
    "        return self.deconv4(self.deconv3(x)).squeeze()\n",
    "\n",
    "class WickUnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import albumentations.augmentations.functional as F\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_names, transform, img_size=False):\n",
    "        self.image_names = image_names\n",
    "        self.transform = transform\n",
    "        self.test = test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(f'prima/{self.image_names[idx]}.tif')\n",
    "        mask = scipy.sparse.load_npz(f'prima/mask_{self.image_names[idx]}.npz').todense()\n",
    "        \n",
    "        transformed = self.transform(image=img, mask=mask)\n",
    "        \n",
    "        if self.img_size:\n",
    "            return transformed[\"image\"], transformed[\"mask\"], img.shape[:2]\n",
    "        return transformed[\"image\"], transformed[\"mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(392, 260),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transform = A.Compose(\n",
    "    [\n",
    "         A.Resize(392, 260),\n",
    "         A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "         ToTensorV2()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files = ['00008152', '00008153', '00008150', '00008148', '00325453',\n",
    "'00008063', '00008230', '00322470', '00325454', '00008332',\n",
    "'00008062', '00008143', '00008344', '00008141', '00008146',\n",
    "'00008088', '00008346', '00325450', '00008149', '00008147',\n",
    "'00325448', '00008227', '00008151', '00322599', '00322596',\n",
    "'00008334', '00008336', '00008089', '00008229', '00322468',\n",
    "'00008144', '00008340', '00008061', '00008084', '00325449',\n",
    "'00322471', '00008140', '00322469', '00008086', '00008145']\n",
    "\n",
    "test_files = ['00008228',\n",
    " '00322597',\n",
    " '00008338',\n",
    " '00008064',\n",
    " '00322598',\n",
    " '00325451',\n",
    " '00008142',\n",
    " '00325452',\n",
    " '00008154',\n",
    " '00008342']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = np.random.choice(training_files, 32, replace=False)\n",
    "val_files = list(set(training_files) - set(train_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ImageDataset(train_files, train_transform)\n",
    "val_ds = ImageDataset(val_files, val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_dl = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, loss_fun, optimizer):\n",
    "    total_loss = 0\n",
    "    total_row = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for img, mask in dataloader:\n",
    "        pred = model(img)\n",
    "        loss = loss_fun(pred, mask.float())\n",
    "\n",
    "        total_loss += loss.item() * img.shape[0]\n",
    "        total_row += img.shape[0]\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / total_row\n",
    "\n",
    "def validate(model, dataloader, loss_fun):\n",
    "    total_loss = 0\n",
    "    total_row = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, mask in dataloader:\n",
    "            pred = model(img)\n",
    "            loss = loss_fun(pred, mask.float())\n",
    "\n",
    "            total_loss += loss.item() * img.shape[0]\n",
    "            total_row += img.shape[0]\n",
    "\n",
    "    return total_loss / total_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WickUnet()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "loss_fun = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = train(model, train_dl, loss_fun, optimizer)\n",
    "vl = validate(model, val_dl, loss_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7293292880058289, 0.6918942332267761)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl, vl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
