{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import albumentations.augmentations.functional as F\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_names, transform, img_size=False):\n",
    "        self.image_names = image_names\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(f'prima/{self.image_names[idx]}.tif')\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = scipy.sparse.load_npz(f'mask/mask_{self.image_names[idx]}.npz').todense()\n",
    "        \n",
    "        transformed = self.transform(image=img, mask=mask)\n",
    "        \n",
    "        if self.img_size:\n",
    "            h, w = img.shape[:2]\n",
    "            return transformed[\"image\"], h, w\n",
    "        return transformed[\"image\"], transformed[\"mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = ['00008228',\n",
    " '00322597',\n",
    " '00008338',\n",
    " '00008064',\n",
    " '00322598',\n",
    " '00325451',\n",
    " '00008142',\n",
    " '00325452',\n",
    " '00008154',\n",
    " '00008342']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels=[3, 40, 60, 120, 160, 240], kernel=5, padding=2, pool_kernel=2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels[0], channels[1], kernel, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(channels[1], channels[2], kernel, padding=padding)\n",
    "        self.conv3 = nn.Conv2d(channels[2], channels[3], kernel, padding=padding)\n",
    "        self.conv4 = nn.Conv2d(channels[3], channels[4], kernel, padding=padding)\n",
    "        self.conv5 = nn.Conv2d(channels[4], channels[5], kernel, padding=padding)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        # setting stride to equal kernel\n",
    "        self.pool = nn.MaxPool2d(pool_kernel, stride=pool_kernel)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv2(self.relu(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.conv4(self.relu(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        return self.relu(self.conv5(x))\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels=[240, 120, 60, 2, 1], kernel=5, padding=2, mid_kernel=2):\n",
    "        super().__init__()\n",
    "        self.deconv1 = nn.ConvTranspose2d(channels[0], channels[1], kernel, padding=padding)\n",
    "        # kernel and stride to match the pool layer in the encoder\n",
    "        self.deconv2 = nn.ConvTranspose2d(channels[1], channels[2], mid_kernel, stride=mid_kernel)\n",
    "        self.deconv3 = nn.ConvTranspose2d(channels[2], channels[3], mid_kernel, stride=mid_kernel)\n",
    "        # for generating output (out channel is 1 mask is one layer)\n",
    "        self.deconv4 = nn.ConvTranspose2d(channels[3], channels[4], kernel, padding=padding)\n",
    "    \n",
    "        self.relu = nn.ReLU()\n",
    "      \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.deconv1(x))\n",
    "        x = self.relu(self.deconv2(x))\n",
    "        return self.deconv4(self.deconv3(x)).squeeze()\n",
    "\n",
    "class WickUnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('rotate_noise_epoch_90.pt', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = A.Compose(\n",
    "    [\n",
    "         A.PadIfNeeded(min_height=10296, min_width=7020),\n",
    "         A.Resize(396, 264),\n",
    "         A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "         ToTensorV2()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ImageDataset(list(test_files), val_transform, img_size=True)\n",
    "val_dl = DataLoader(ds, batch_size = 1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for i,(img, h,w) in enumerate(val_dl):\n",
    "        pred = model(img)\n",
    "        \n",
    "        probabilities = torch.sigmoid(pred)\n",
    "        predicted_masks = (probabilities >= 0.5).float().detach().numpy()\n",
    "\n",
    "        aug = A.Compose(\n",
    "            [\n",
    "                A.Resize(10296, 7020),\n",
    "                A.CenterCrop(h, w)\n",
    "            ]\n",
    "        )\n",
    "        del probabilities\n",
    "        transformed = aug(image=predicted_masks)\n",
    "        \n",
    "        sparse_mask = scipy.sparse.csr_matrix(transformed['image'])\n",
    "        scipy.sparse.save_npz(f'mask/pred_mask_{test_files[i]}.npz', sparse_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18110487719112883"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = 0\n",
    "for file in test_files:\n",
    "    y = scipy.sparse.load_npz(f'prima/mask_{file}.npz').todense()\n",
    "    pred = scipy.sparse.load_npz(f'Archive/pred_mask_{file}.npz').todense()\n",
    "    f1 += f1_score(np.array(y).flatten(),np.array(pred).flatten().astype(int))\n",
    "f1 = f1/len(test_files)    \n",
    "f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
